# Computer-Vision-Rock-Paper-Scissors

## Overview

This is a game project that involves creating a deep learning model using a computer vision software called Teacheable Machine and a neutral network to train images of hand gestures that indicate Rock, Paper, Scissors (and nothing) to use in the game of the same name. Tensorflow and Python are the technologies used for this project.

## Milestone 1: Create the Model from Teacheable Machine

- Four classes are created from Teacheable Machine, an open-source computer vision software that takes images using the laptop camera as inputs to represent the different classes: Rock, Paper, Scissors and Nothing.

- Around 2000-2500 images are taken for each class, making as many variations of each hand gesture as possible to maximise the effectiveness of the model during the training phase so that it can accurately predict the class for new images. 
- Many of the variations of each class are different hand placements with respect to the head, using different hands and different ways to show a rock, paper or scissors gesture from a camera's perspective. This will increase the likelihood of the model getting the class right.

